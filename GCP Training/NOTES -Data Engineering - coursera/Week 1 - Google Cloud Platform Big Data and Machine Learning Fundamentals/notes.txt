=========Google Cloud Platform Intro============

==========WEEK 1==============================

Long term shift:
First Wave
    - Computers under desks
Second Wave
    - Data Centers
Third Wave
    - Virtual data centers
    - Pay per provision

Google's mission is to organize the world's information
Google buys 1 out of 5 CPUs on the planet - wow!

The cloud is most beneficial when you run things just when you need them.

You can view GCP Big Data Products by functionality 
    Foundation
        Compute Engine
        Cloud Storage
    Databases
        Datastore
        Cloud SQL
        Cloud Bigtable
    Analytics and ML
        BigQuery
        Cloud Datalab
        Translation API, Vision API, ...
    Data-handling frameworks
        Cloud pub/sub
        Cloud Dataflow
        Cloud Dataproc
        
No-Ops = No systems engineer required. Simply submit jobs.

Journey to the cloud: When customers join GCP they
    Change where they compute
        Faster & Cheaper
    Improve scalability and reliability
        messaging -> pub/sub
        data processing -> dataflow
                                      or spark w/ dataproc (resize clusters very quickly)
    Change how they compute
    
Google Cloud offers ways to...
    Spend less on ops and administration
    Incorporate real-time data into apps and architectures
    Apply ML broadly and easily
    Become a data-driven company
    
pre-emptable machines offer 80% discount (the opposite of fault-tolerant)
    For instance: You can have 10 standard VMs and 30 pre-emptable VMs
    whereas 10 can get the job done bare minimum, but the extra pre-emptables improve the service greatly
    
Lab 1 Create a compute engine instance
    Create Compute Engine instance with the necessary API access
    Open hamburger menu and create Compute Engine
        view machine info:
            cat /proc/cpuinfo
        update
            sudo apt-get update
            sudo apt-get -y -qq install git
            git --version
            exit
            
A Global Filesystem
    Use Google Cloud Storage for persistent storage
        and as a staging ground for import to other Google Cloud products
            ie: Cloud SQL, BigQuery, Dataproc
        This is how you just upload files in a basic, raw, staged way. They can "do" much of anything here.
        ie: blob storage
        Could be populated from a compute engine.
            Compute engines might have their own disk for fast writing
            Often first step of data cycle is to go from disk to GC Storage
        gsutil co MY_FILENAME*.csv gs://MY_BUCKET1/data/
            bucket names must be unique & if it's a company name you need to prove you own it.
        Not an actual folder, not hierarchal, pure key value store
            However, you can ls the "directory"
        gsutil mb = make bucket
        gsutil rb = remove bucket
        gsutil rsync = mirror local folder and upload files which have changed
        gsutil = invokes a web service REST API (which you can do yourself)
        --ingest--> Compute Engine --store--> Cloud Storage ----> Cloud SQL OR publish
        You can control access at the project level
            All editors can change this bucket
            All authenticated google users can read from this bucket
            All users can read this data
                Treat as a content delivery edge-cached, replicated, durable data sharing system
    distributing applications and data across several zones for redundancy if users are in one country
    Distribute across regions to minimize impact of service disruptions
    Use multiple regions to provide global access to your application.
        
Lab 2: Earthquake data to map with Cloud Storage and make it publicly available
    1) credential in
    2) make a compute engine & Allow full access to all Cloud APIs
    3) SSH in and get some data
        sudo apt-get update
        sudo apt-get -y -qq install git
        git clone https://github.com/GoogleCloudPlatform/training-data-analyst
        cd training-data-analyst/CPB100/lab2b
        bash ingest.sh
        head earthquakes.csv
    4) transform the data
        Read the data narrative: https://github.com/GoogleCloudPlatform/datalab-samples/blob/master/basemap/earthquakes.ipynb
        bash install_missing.sh
        python3 transform.py
        ls -l
    5) Make bucket
        hamburger menu ->storage -> Create Bucket with PROJECT_ID
    6) Move data from URL to storage bucket
        gsutil cp earthquakes.* gs://PROJECT_ID/earthquakes/
        witness three
    7) Publish storage bucket to web
        gsutil acl ch -u AllUsers:R gs://<YOUR-BUCKET>/earthquakes/*
       
Quiz:
    Compute nodes on GCP are:
        -Allocated on demand, and you pay for the time that they are up.
        Expensive to create and teardown
        Pre-installed with all the software packages you might ever need.
        One of ~50 choices in terms of CPU and memory
    Google Cloud Storage is a good option for storing data that:
    (Select all 2 of the correct options)
        Is ingested in real-time from sensors and other devices
        Will be frequently read/written from a compute node
        -May be required to be read at some later time
        -May be imported into a cluster for analysis
        
Compute Engines should be fungible, something that comes and goes. It's not expensive.

check out https://cloud.google.com/pricing and see that you can type in:
  COMPUTE:
    N instances
    M SSDs
    H hours per day
    D days per week
  STORAGE:
    ...
  and get an estimate
  
check out cloud launcher @ hamburger menu OR https://cloud.google.com/launcher
    select from pre-made machines that are for wordpress or any other use-cases
        These are just configuration files which make some good templates to read
        ^------- THIS SOUNDS LIKE A GOOD TIP

Intro to Managed Services for Common Use Cases

Stepping Stones to Transformation
    recommendation system 3 components:
        1)   Rating
        2)  Training
        3)  Recommending
        This guy is seriously rambling about recommendation engines for 10 minutes and straining not to say the words "collaborative filter" >:( What does this have to do with Stepping Stones to Transformation? I would just abandon the "story" that we are on-boarding someone and just present it as a separate video introducing a case study where we are going to look at "python spark on dataproc"
        
        

-------------------- EOD 1/17/2019 -------------------